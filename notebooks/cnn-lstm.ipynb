{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d16ab8d",
   "metadata": {},
   "source": [
    "## 1. Load & preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "ds = load_from_disk(\"/kaggle/input/handwriting/data/mathwriting\")\n",
    "num_samples = 40000\n",
    "ds_train = ds[\"train\"].select(range(num_samples))\n",
    "\n",
    "# 2. Pre-allocate Image Array (Saves RAM by avoiding copies)\n",
    "print(f\"Pre-allocating memory for {num_samples} images...\")\n",
    "images_array = np.zeros((num_samples, 256, 256), dtype=np.float32)\n",
    "latex_strings = []\n",
    "\n",
    "# 3. Process Images & Collect Strings\n",
    "print(\"Processing images and LaTeX strings...\")\n",
    "for i in range(num_samples):\n",
    "    sample = ds_train[i]\n",
    "    # Convert and resize directly into the array\n",
    "    img = sample[\"image\"].convert(\"L\").resize((256, 256))\n",
    "    images_array[i] = np.array(img, dtype=np.float32) / 255.0\n",
    "    latex_strings.append(sample[\"latex\"])\n",
    "    \n",
    "    if (i + 1) % 5000 == 0:\n",
    "        print(f\"Progress: {i + 1}/{num_samples}\")\n",
    "\n",
    "# 4. Setup Tokenizer\n",
    "print(\"Fitting tokenizer...\")\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(latex_strings)\n",
    "\n",
    "# Add special tokens\n",
    "tokenizer.word_index[\"<START>\"] = len(tokenizer.word_index) + 1\n",
    "tokenizer.word_index[\"<END>\"] = len(tokenizer.word_index) + 1\n",
    "tokenizer.index_word[tokenizer.word_index[\"<START>\"]] = \"<START>\"\n",
    "tokenizer.index_word[tokenizer.word_index[\"<END>\"]] = \"<END>\"\n",
    "\n",
    "START_ID = tokenizer.word_index[\"<START>\"]\n",
    "END_ID   = tokenizer.word_index[\"<END>\"]\n",
    "\n",
    "# 5. Sequence Padding\n",
    "print(\"Tokenizing and padding sequences...\")\n",
    "sequences = tokenizer.texts_to_sequences(latex_strings)\n",
    "sequences = [[START_ID] + seq + [END_ID] for seq in sequences]\n",
    "padded_sequences = pad_sequences(sequences, padding=\"post\")\n",
    "\n",
    "# 6. Save Tokenizer and Vocab Info\n",
    "print(\"Saving metadata...\")\n",
    "with open(\"/kaggle/working/latex_tokenizer256.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "with open(\"/kaggle/working/vocab_size.txt\", \"w\") as f:\n",
    "    f.write(str(vocab_size))\n",
    "\n",
    "# 7. Convert to Tensors and Save (Disk usage check: ~13.5GB total)\n",
    "print(\"converting to Tensors...\")\n",
    "# torch.from_numpy avoids a RAM copy\n",
    "images_tensor = torch.from_numpy(images_array).unsqueeze(1) \n",
    "tokens_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "print(\"saving tensors to disk...\")\n",
    "torch.save(images_tensor, \"/kaggle/working/images_train256.pt\")\n",
    "torch.save(tokens_tensor, \"/kaggle/working/tokens_train256.pt\")\n",
    "\n",
    "print(\"Done!\")\n",
    "print(f\"Final Vocab Size: {vocab_size}\")\n",
    "print(f\"Image Tensor Shape: {images_tensor.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"/kaggle/input/handwriting/data/mathwriting\")\n",
    "\n",
    "ds_val = ds[\"val\"].select(range(5000))\n",
    "\n",
    "images, sequences = [], []\n",
    "\n",
    "def preprocess_image(img, target_size=(256, 256)):\n",
    "    img = img.convert(\"L\")  # convert to grayscale\n",
    "    img = img.resize(target_size)\n",
    "    img = np.array(img) / 255.0  # normalize to [0, 1]\n",
    "    return img\n",
    "\n",
    "for sample in ds_val:\n",
    "    img = preprocess_image(sample[\"image\"])\n",
    "    images.append(img)\n",
    "    sequences.append(sample[\"latex\"])\n",
    "\n",
    "images = np.array(images)\n",
    "with open(\"/kaggle/working/latex_tokenizer256.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "START_ID = tokenizer.word_index[\"<START>\"]\n",
    "END_ID   = tokenizer.word_index[\"<END>\"]\n",
    "\n",
    "seqs = tokenizer.texts_to_sequences(sequences)\n",
    "seqs = [[START_ID] + s + [END_ID] for s in seqs]\n",
    "\n",
    "padded_sequences = pad_sequences(seqs, padding=\"post\")\n",
    "images = images[..., np.newaxis]\n",
    "\n",
    "images_tensor = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "tokens_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "torch.save(images_tensor, \"/kaggle/working/images_val256.pt\")\n",
    "torch.save(tokens_tensor, \"/kaggle/working/tokens_val256.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d33b6",
   "metadata": {},
   "source": [
    "## 2. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf058c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  # grayscale input\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, embedding_dim, 3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))  # each conv layer extracts complex features\n",
    "        x = self.pool1(x)  # each pool layer reduces spatial dimensions, keeping important features\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 3, 1) # (B, 32, 32, 128)\n",
    "        x = x.view(B, -1, C)      # (B, 1024, 128)\n",
    "        return x  \n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad30c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):  # additive attention mechanism\n",
    "    def __init__(self, enc_dim, dec_units):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(enc_dim, dec_units)  # to transform encoder features to same dimension as decoder hidden state\n",
    "        self.W2 = nn.Linear(dec_units, dec_units)\n",
    "        self.V = nn.Linear(dec_units, 1)  # to get attention scores for each encoder feature\n",
    "    def forward(self, encoder_features, hidden):\n",
    "        # hidden: (B, dec_units) -> (B, 1, dec_units) to broadcast\n",
    "        hidden_with_time = hidden.unsqueeze(1)\n",
    "        # score: (B, seq_len, 1)\n",
    "        score = self.V(torch.tanh(self.W1(encoder_features) + self.W2(hidden_with_time)))\n",
    "        # attention_weights: (B, seq_len, 1)\n",
    "        attention_weights = F.softmax(score, dim=1)  # softmax over seq_len to get weights that sum to 1\n",
    "        # context_vector: weighted sum over encoder features\n",
    "        context_vector = torch.sum(attention_weights * encoder_features, dim=1)  # (B, enc_dim)\n",
    "        # context_vector is the weighted average of encoder features\n",
    "        return context_vector, attention_weights  # attention_weights show how much each encoder feature contributed to the context vector\n",
    "\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, rnn_units=512, enc_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # map token ids to embeddings\n",
    "        self.attention = Attention(enc_dim, rnn_units)  # attention mechanism to focus on relevant encoder features\n",
    "        self.lstm = nn.LSTM(embedding_dim + enc_dim, rnn_units, batch_first=True)  # embedding_dim + enc_dim so that it can see the image features at each time step\n",
    "        self.enc_to_h = nn.Linear(enc_dim, rnn_units)  # change encoder features to initial hidden state size\n",
    "        self.fc = nn.Linear(rnn_units, vocab_size)  # final output: logits for each token, size vocab_size\n",
    "\n",
    "    def forward(self, x, encoder_features, hidden=None):\n",
    "        B, T = x.shape\n",
    "        x_embed = self.embedding(x)  # turn token IDs into embeddings\n",
    "        outputs = []\n",
    "        if hidden is None:\n",
    "            mean_features = encoder_features.mean(dim = 1)\n",
    "            h0 = torch.tanh(self.enc_to_h(mean_features)).unsqueeze(0)  # map encoder features to initial hidden state\n",
    "            c0 = torch.zeros_like(h0)  # initial cell state\n",
    "            hidden = (h0, c0)\n",
    "\n",
    "        h_t, c_t = hidden\n",
    "        for t in range(T):\n",
    "            x_t = x_embed[:, t, :]  # (B, embedding_dim)\n",
    "            context, attn_weights = self.attention(encoder_features, h_t.squeeze(0))\n",
    "            lstm_input = torch.cat([x_t, context], dim=-1).unsqueeze(1)\n",
    "            out, (h_t, c_t) = self.lstm(lstm_input, (h_t, c_t))\n",
    "            outputs.append(out)\n",
    "        output = torch.cat(outputs, dim=1)  # (B, T, rnn_units)\n",
    "        logits = self.fc(output)  # (batch, seq_len, vocab_size)\n",
    "        # state_h is the final hidden state of the current output and state_c is a memory to remember important info from previous tokens\n",
    "        return logits, (h_t, c_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b14993",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7847e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "embedding_dim = 128\n",
    "rnn_units = 1024\n",
    "with open(\"/kaggle/working/vocab_size.txt\") as f:\n",
    "    VOCAB_SIZE = int(f.read().strip())\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "learning_rate = 0.0005\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = CNNEncoder(embedding_dim=embedding_dim).to(DEVICE)\n",
    "decoder = RNNDecoder(vocab_size=VOCAB_SIZE, embedding_dim=embedding_dim, rnn_units=rnn_units, enc_dim = embedding_dim).to(DEVICE)\n",
    "\n",
    "images = torch.load(\"/kaggle/working/images_train256.pt\")  # (N, 1, 128, 128)\n",
    "tokens = torch.load(\"/kaggle/working/tokens_train256.pt\")  # (N, seq_len)\n",
    "\n",
    "dataset = TensorDataset(images, tokens)  # pair images and token sequences\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)  # creates an iterable over dataset\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding index 0\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())  # get all trainable parameters (weights and biases)\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, seqs in loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        seqs = seqs.to(DEVICE)\n",
    "\n",
    "        # teacher forcing, use ground truth tokens as input instead of previous predictions so that model learns to predict next token better\n",
    "        input_tokens = seqs[:, :-1]   # (B, seq_len-1), takes all but last token\n",
    "        target_tokens = seqs[:, 1:]   # (B, seq_len-1), takes all but first token\n",
    "\n",
    "        optimizer.zero_grad()  # clear previous gradients\n",
    "\n",
    "        # encode images\n",
    "        image_features = encoder(imgs)  # (B, 256)\n",
    "\n",
    "        # decode sequences, pass encoder_features for initial hidden\n",
    "        logits, _ = decoder(input_tokens, encoder_features=image_features)  # logits: (B, seq_len-1, vocab_size)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, VOCAB_SIZE),  # (B * (seq_len-1), vocab_size)\n",
    "            target_tokens.reshape(-1)  # (B * (seq_len-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()  # backpropagate\n",
    "        optimizer.step()  # update weights\n",
    "\n",
    "        total_loss += loss.item()  # accumulate loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} | Loss: {total_loss / len(loader):.4f}\")\n",
    "    # if (epoch + 1) % 1 == 0:\n",
    "    #   torch.save({\n",
    "    #       \"encoder\": encoder.state_dict(),\n",
    "    #       \"decoder\": decoder.state_dict(),\n",
    "    #   }, f\"/content/drive/MyDrive/ColabNotebooks/handtex_epoch_{epoch+1}.pt\")\n",
    "\n",
    "SAVE_PATH = \"/kaggle/working/handtex_model256.pt\"\n",
    "torch.save({\n",
    "    \"encoder\": encoder.state_dict(),\n",
    "    \"decoder\": decoder.state_dict()\n",
    "}, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6588e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "images_train = torch.load(\"/kaggle/working/images_train256.pt\")\n",
    "tokens_train = torch.load(\"/kaggle/working/tokens_train256.pt\")\n",
    "images_val = torch.load(\"/kaggle/working/images_val256.pt\")\n",
    "tokens_val = torch.load(\"/kaggle/working/tokens_val256.pt\")\n",
    "\n",
    "VOCAB_SIZE = int(open(\"/kaggle/working/vocab_size.txt\").read().strip())\n",
    "\n",
    "train_dataset = TensorDataset(images_train, tokens_train)\n",
    "val_dataset = TensorDataset(images_val, tokens_val)\n",
    "\n",
    "param_grid = {\n",
    "    \"embedding_dim\": [256],      \n",
    "    \"rnn_units\": [512],          \n",
    "    \"learning_rate\": [0.0005],  \n",
    "    \"batch_size\": [16]     \n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 7\n",
    " \n",
    "all_combinations = list(itertools.product(\n",
    "    param_grid[\"embedding_dim\"],\n",
    "    param_grid[\"rnn_units\"],\n",
    "    param_grid[\"learning_rate\"],\n",
    "    param_grid[\"batch_size\"]\n",
    "))\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "for emb_dim, rnn_units, lr, batch_size in all_combinations:\n",
    "    print(f\"\\nTesting: emb_dim={emb_dim}, rnn_units={rnn_units}, lr={lr}, batch_size={batch_size}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    encoder = CNNEncoder(embedding_dim=emb_dim).to(DEVICE)\n",
    "    decoder = RNNDecoder(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embedding_dim=emb_dim,\n",
    "        rnn_units=rnn_units,\n",
    "        enc_dim=emb_dim\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(encoder.parameters()) + list(decoder.parameters()),\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    best_epoch_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # training\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for imgs, seqs in train_loader:\n",
    "            imgs, seqs = imgs.to(DEVICE), seqs.to(DEVICE)\n",
    "            input_tokens, target_tokens = seqs[:, :-1], seqs[:, 1:]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            features = encoder(imgs)\n",
    "            logits, _ = decoder(input_tokens, encoder_features=features)\n",
    "\n",
    "            loss = criterion(\n",
    "                logits.reshape(-1, VOCAB_SIZE),\n",
    "                target_tokens.reshape(-1)\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # validation\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, seqs in val_loader:\n",
    "                imgs, seqs = imgs.to(DEVICE), seqs.to(DEVICE)\n",
    "                input_tokens, target_tokens = seqs[:, :-1], seqs[:, 1:]\n",
    "\n",
    "                features = encoder(imgs)\n",
    "                logits, _ = decoder(input_tokens, encoder_features=features)\n",
    "\n",
    "                loss = criterion(\n",
    "                    logits.reshape(-1, VOCAB_SIZE),\n",
    "                    target_tokens.reshape(-1)\n",
    "                )\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        best_epoch_val_loss = min(best_epoch_val_loss, val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{EPOCHS} | \"\n",
    "            f\"Train: {train_loss:.4f} | Val: {val_loss:.4f}\"\n",
    "        )\n",
    "    if best_epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = best_epoch_val_loss\n",
    "        best_params = {\n",
    "            \"embedding_dim\": emb_dim,\n",
    "            \"rnn_units\": rnn_units,\n",
    "            \"learning_rate\": lr,\n",
    "            \"batch_size\": batch_size\n",
    "        }\n",
    "\n",
    "    del encoder, decoder, optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nBest params:\", best_params)\n",
    "print(\"Best val loss:\", best_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb34d23",
   "metadata": {},
   "source": [
    "## 4. Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85188bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pickle import load\n",
    "import sys\n",
    "from pathlib import Path\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(\"/kaggle/working/vocab_size.txt\") as f:\n",
    "    VOCAB_SIZE = int(f.read().strip())\n",
    "\n",
    "START_TOKEN = VOCAB_SIZE - 2\n",
    "END_TOKEN = VOCAB_SIZE - 1\n",
    "MAX_LEN = 150\n",
    "\n",
    "# Load models\n",
    "encoder = CNNEncoder().to(DEVICE)\n",
    "\n",
    "decoder = RNNDecoder(\n",
    "    vocab_size=66, \n",
    "    embedding_dim=128,  # match trained model\n",
    "    rnn_units=1024,      # match trained model\n",
    "    enc_dim=128          # match trained model\n",
    ")\n",
    "encoder.to(DEVICE) \n",
    "decoder.to(DEVICE) \n",
    "\n",
    "checkpoint = torch.load(\"/kaggle/input/datasets/martinvu7/cnn-lstm2/handtex_model256.pt\", map_location=DEVICE)  # load trained weights\n",
    "encoder.load_state_dict(checkpoint[\"encoder\"])  # all learned weights and biases\n",
    "decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "\n",
    "encoder.eval()  # set to evaluation mode\n",
    "decoder.eval()\n",
    "\n",
    "images = torch.load(\"/kaggle/working/images_val256.pt\")\n",
    "tokens = torch.load(\"/kaggle/working/tokens_val256.pt\")\n",
    "\n",
    "with open(\"/kaggle/working/latex_tokenizer256.pkl\", \"rb\") as f:\n",
    "    tokenizer = load(f)\n",
    "\n",
    "inv_vocab = {v: k for k, v in tokenizer.word_index.items()}  # reverse mapping\n",
    "def decode(seq):\n",
    "    return \"\".join(inv_vocab.get(t, \"\") for t in seq)\n",
    "\n",
    "# define NED function somewhere above\n",
    "def normalized_edit_distance(pred, target):\n",
    "    m, n = len(pred), len(target)\n",
    "    if m == 0 and n == 0:\n",
    "        return 0.0\n",
    "    dp = list(range(n + 1))\n",
    "    for i in range(1, m + 1):\n",
    "        prev = dp[0]\n",
    "        dp[0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            temp = dp[j]\n",
    "            if pred[i - 1] == target[j - 1]:\n",
    "                dp[j] = prev\n",
    "            else:\n",
    "                dp[j] = 1 + min(dp[j], dp[j - 1], prev)\n",
    "            prev = temp\n",
    "    return dp[n] / max(m, n)\n",
    "\n",
    "# inference\n",
    "N = 500  # how many samples to test\n",
    "total = 0\n",
    "for i in range(N):\n",
    "    img = images[i:i+1].to(DEVICE)  # keep batch dimension\n",
    "    gt_tokens = tokens[i]\n",
    "    with torch.no_grad():\n",
    "        features = encoder(img)\n",
    "        input_token = torch.tensor([[START_TOKEN]], device=DEVICE)\n",
    "        hidden = None\n",
    "        pred_tokens = []\n",
    "\n",
    "        for _ in range(MAX_LEN):\n",
    "            logits, hidden = decoder(input_token, features, hidden=hidden)\n",
    "            next_token = logits.argmax(-1)  # index of highest logit\n",
    "            token_id = next_token.item() # get scalar token ID\n",
    "            if token_id == END_TOKEN:\n",
    "                break\n",
    "            pred_tokens.append(token_id)\n",
    "            input_token = next_token  # feed predicted token back\n",
    "\n",
    "    gt_str = decode(gt_tokens.tolist())\n",
    "    pred_str = decode(pred_tokens)\n",
    "    ned = normalized_edit_distance(pred_str, gt_str)\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"GT:  \", gt_str)\n",
    "    print(\"PRED:\", pred_str)\n",
    "    print(\"NED: \", ned)\n",
    "    print(\"-\"*40)\n",
    "    total += ned\n",
    "print(total / 500)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
