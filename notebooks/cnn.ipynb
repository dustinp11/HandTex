{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Encoder + LSTM Decoder\n",
    "Custom 3-layer CNN encoder with LSTM decoder for handwritten math to LaTeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dustp\\anaconda3\\envs\\cs175_gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"deepcopy/MathWriting-human\")\n",
    "print(f\"Train: {len(ds['train'])}  Val: {len(ds['val'])}  Test: {len(ds['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to None to use ALL training data\n",
    "NUM_TRAIN = 2000\n",
    "\n",
    "ds_train = ds[\"train\"] if NUM_TRAIN is None else ds[\"train\"].select(range(NUM_TRAIN))\n",
    "print(f\"Using {len(ds_train)} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "\n",
    "images, latex_strings = [], []\n",
    "for sample in ds_train:\n",
    "    img = sample[\"image\"].convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n",
    "    images.append(np.array(img) / 255.0)\n",
    "    latex_strings.append(sample[\"latex\"])\n",
    "\n",
    "images = np.array(images)\n",
    "print(f\"Images: {images.shape}\")\n",
    "\n",
    "# char-level tokenizer\n",
    "chars = sorted(set(\"\".join(latex_strings)))\n",
    "word_index = {ch: i + 1 for i, ch in enumerate(chars)}\n",
    "idx_to_char = {v: k for k, v in word_index.items()}\n",
    "\n",
    "sequences = [[word_index[ch] for ch in s] for s in latex_strings]\n",
    "max_len = max(len(s) for s in sequences)\n",
    "padded = np.array([s + [0] * (max_len - len(s)) for s in sequences])\n",
    "\n",
    "VOCAB_SIZE = len(word_index) + 1\n",
    "print(f\"Vocab size: {VOCAB_SIZE}, Max seq len: {max_len}\")\n",
    "\n",
    "images_tensor = torch.tensor(images, dtype=torch.float32).unsqueeze(1)  # (N, 1, H, W)\n",
    "tokens_tensor = torch.tensor(padded, dtype=torch.long)\n",
    "print(f\"Images tensor: {images_tensor.shape}\")\n",
    "print(f\"Tokens tensor: {tokens_tensor.shape}\")\n",
    "\n",
    "dataset = TensorDataset(images_tensor, tokens_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, encoded_dim=256):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, encoded_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        features = self.adaptive_pool(features)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.fc(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, encoder_out, targets):\n",
    "        embeddings = self.embedding(targets)\n",
    "        h0 = encoder_out.unsqueeze(0)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        outputs, _ = self.lstm(embeddings, (h0, c0))\n",
    "        logits = self.fc(outputs)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = CNNEncoder(encoded_dim=hidden_dim)\n",
    "        self.decoder = LSTMDecoder(vocab_size, embed_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, images, targets):\n",
    "        features = self.encoder(images)\n",
    "        logits = self.decoder(features, targets)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, image, max_len=100, sos_idx=1, eos_idx=2):\n",
    "        self.eval()\n",
    "        features = self.encoder(image)\n",
    "        h = features.unsqueeze(0)\n",
    "        c = torch.zeros_like(h)\n",
    "        token = torch.tensor([[sos_idx]], device=image.device)\n",
    "        output_tokens = []\n",
    "        for _ in range(max_len):\n",
    "            emb = self.decoder.embedding(token)\n",
    "            out, (h, c) = self.decoder.lstm(emb, (h, c))\n",
    "            logits = self.decoder.fc(out)\n",
    "            next_token = logits.argmax(dim=-1)\n",
    "            if next_token.item() == eos_idx:\n",
    "                break\n",
    "            output_tokens.append(next_token.item())\n",
    "            token = next_token\n",
    "        return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = CNNLSTMModel(vocab_size=VOCAB_SIZE).to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total params: {total_params:,} (all trainable)\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = Path(\"checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (imgs, seqs) in enumerate(loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        seqs = seqs.to(DEVICE)\n",
    "\n",
    "        input_tokens = seqs[:, :-1]\n",
    "        target_tokens = seqs[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs, input_tokens)\n",
    "        loss = criterion(logits.reshape(-1, VOCAB_SIZE), target_tokens.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": avg_loss,\n",
    "    }, checkpoint_dir / f\"cnn_epoch_{epoch+1}.pt\")\n",
    "\n",
    "torch.save(model.state_dict(), checkpoint_dir / \"cnn_final.pt\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, word_index):\n",
    "        self.word_index = word_index\n",
    "\n",
    "with open(\"latex_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(CharTokenizer(word_index), f)\n",
    "\n",
    "with open(\"vocab_size.txt\", \"w\") as f:\n",
    "    f.write(str(VOCAB_SIZE))\n",
    "\n",
    "print(f\"Saved tokenizer and vocab_size={VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_edit_distance(pred, target):\n",
    "    m, n = len(pred), len(target)\n",
    "    if m == 0 and n == 0:\n",
    "        return 0.0\n",
    "    dp = list(range(n + 1))\n",
    "    for i in range(1, m + 1):\n",
    "        prev = dp[0]\n",
    "        dp[0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            temp = dp[j]\n",
    "            if pred[i - 1] == target[j - 1]:\n",
    "                dp[j] = prev\n",
    "            else:\n",
    "                dp[j] = 1 + min(dp[j], dp[j - 1], prev)\n",
    "            prev = temp\n",
    "    return dp[n] / max(m, n)\n",
    "\n",
    "\n",
    "NUM_EVAL = 50\n",
    "SEED = 42\n",
    "\n",
    "test_ds = ds[\"test\"]\n",
    "random.seed(SEED)\n",
    "eval_indices = random.sample(range(len(test_ds)), NUM_EVAL)\n",
    "eval_samples = [test_ds[i] for i in eval_indices]\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "model.eval()\n",
    "exact_matches = 0\n",
    "total_edit_dist = 0.0\n",
    "\n",
    "print(f\"Evaluating on {NUM_EVAL} test samples...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, sample in enumerate(eval_samples):\n",
    "    ground_truth = sample[\"latex\"]\n",
    "    img = sample[\"image\"].convert(\"L\")\n",
    "    img_tensor = eval_transform(img).unsqueeze(0).to(DEVICE)  # (1, 1, 128, 128)\n",
    "\n",
    "    pred_tokens = model.generate(img_tensor)\n",
    "    prediction = \"\".join(idx_to_char.get(t, \"?\") for t in pred_tokens)\n",
    "\n",
    "    is_exact = prediction == ground_truth\n",
    "    edit_dist = normalized_edit_distance(prediction, ground_truth)\n",
    "\n",
    "    if is_exact:\n",
    "        exact_matches += 1\n",
    "    total_edit_dist += edit_dist\n",
    "\n",
    "    status = \"EXACT\" if is_exact else f\"edit_dist={edit_dist:.4f}\"\n",
    "    print(f\"  [{i+1}/{NUM_EVAL}] {status}\")\n",
    "    print(f\"    GT:   {ground_truth[:80]}\")\n",
    "    print(f\"    Pred: {prediction[:80]}\")\n",
    "\n",
    "accuracy = exact_matches / NUM_EVAL\n",
    "avg_edit_dist = total_edit_dist / NUM_EVAL\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model:                    CNN + LSTM\")\n",
    "print(f\"Samples:                  {NUM_EVAL}\")\n",
    "print(f\"Exact match accuracy:     {accuracy:.2%} ({exact_matches}/{NUM_EVAL})\")\n",
    "print(f\"Avg normalized edit dist: {avg_edit_dist:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs175_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
