{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14789329,"sourceType":"datasetVersion","datasetId":9454867}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ViT Encoder + LSTM Decoder Training\nFrozen `google/vit-base-patch16-224` encoder with a trainable LSTM decoder for handwritten math â†’ LaTeX.","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T22:27:17.296435Z","iopub.execute_input":"2026-02-10T22:27:17.296844Z","iopub.status.idle":"2026-02-10T22:27:18.636416Z","shell.execute_reply.started":"2026-02-10T22:27:17.296808Z","shell.execute_reply":"2026-02-10T22:27:18.635397Z"}},"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport random\nimport json\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import transforms\nfrom transformers import ViTModel\nfrom pathlib import Path\nfrom datasets import load_from_disk\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T23:12:06.953982Z","iopub.execute_input":"2026-02-10T23:12:06.954489Z","iopub.status.idle":"2026-02-10T23:12:06.960576Z","shell.execute_reply.started":"2026-02-10T23:12:06.954457Z","shell.execute_reply":"2026-02-10T23:12:06.959476Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 1. Load & preprocess dataset","metadata":{}},{"cell_type":"code","source":"for f in sorted(Path(\"/kaggle/input/datasets\").rglob(\"*\")):\n    print(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T22:44:36.074850Z","iopub.execute_input":"2026-02-10T22:44:36.075525Z","iopub.status.idle":"2026-02-10T22:44:36.097583Z","shell.execute_reply.started":"2026-02-10T22:44:36.075494Z","shell.execute_reply":"2026-02-10T22:44:36.096801Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets/dustinp11\n/kaggle/input/datasets/dustinp11/mathwriting\n/kaggle/input/datasets/dustinp11/mathwriting/dataset_dict.json\n/kaggle/input/datasets/dustinp11/mathwriting/images_train.pt\n/kaggle/input/datasets/dustinp11/mathwriting/test\n/kaggle/input/datasets/dustinp11/mathwriting/test/data-00000-of-00001.arrow\n/kaggle/input/datasets/dustinp11/mathwriting/test/dataset_info.json\n/kaggle/input/datasets/dustinp11/mathwriting/test/state.json\n/kaggle/input/datasets/dustinp11/mathwriting/tokens_train.pt\n/kaggle/input/datasets/dustinp11/mathwriting/train\n/kaggle/input/datasets/dustinp11/mathwriting/train/data-00000-of-00003.arrow\n/kaggle/input/datasets/dustinp11/mathwriting/train/data-00001-of-00003.arrow\n/kaggle/input/datasets/dustinp11/mathwriting/train/data-00002-of-00003.arrow\n/kaggle/input/datasets/dustinp11/mathwriting/train/dataset_info.json\n/kaggle/input/datasets/dustinp11/mathwriting/train/state.json\n/kaggle/input/datasets/dustinp11/mathwriting/val\n/kaggle/input/datasets/dustinp11/mathwriting/val/data-00000-of-00001.arrow\n/kaggle/input/datasets/dustinp11/mathwriting/val/dataset_info.json\n/kaggle/input/datasets/dustinp11/mathwriting/val/state.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(\"Loading dataset...\")\nds = load_from_disk(\"/kaggle/input/datasets/dustinp11/mathwriting\")\nnum_samples = 40000\nds_train = ds[\"train\"].select(range(num_samples))\n\n# 2. Pre-allocate Image Array (Saves RAM by avoiding copies)\n# 50,000 * 256 * 256 * 4 bytes = ~13.1 GB\nprint(f\"Pre-allocating memory for {num_samples} images...\")\nimages_array = np.zeros((num_samples, 256, 256), dtype=np.float32)\nlatex_strings = []\n\n# 3. Process Images & Collect Strings\nprint(\"Processing images and LaTeX strings...\")\nfor i in range(num_samples):\n    sample = ds_train[i]\n    # Convert and resize directly into the array\n    img = sample[\"image\"].convert(\"L\").resize((256, 256))\n    images_array[i] = np.array(img, dtype=np.float32) / 255.0\n    latex_strings.append(sample[\"latex\"])\n    \n    if (i + 1) % 5000 == 0:\n        print(f\"Progress: {i + 1}/{num_samples}\")\n\n# 4. Setup Tokenizer\nprint(\"Fitting tokenizer...\")\ntokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts(latex_strings)\n\n# Add special tokens\ntokenizer.word_index[\"<START>\"] = len(tokenizer.word_index) + 1\ntokenizer.word_index[\"<END>\"] = len(tokenizer.word_index) + 1\ntokenizer.index_word[tokenizer.word_index[\"<START>\"]] = \"<START>\"\ntokenizer.index_word[tokenizer.word_index[\"<END>\"]] = \"<END>\"\n\nSTART_ID = tokenizer.word_index[\"<START>\"]\nEND_ID   = tokenizer.word_index[\"<END>\"]\n\n# 5. Sequence Padding\nprint(\"Tokenizing and padding sequences...\")\nsequences = tokenizer.texts_to_sequences(latex_strings)\nsequences = [[START_ID] + seq + [END_ID] for seq in sequences]\npadded_sequences = pad_sequences(sequences, padding=\"post\")\n\n# 6. Save Tokenizer and Vocab Info\nprint(\"Saving metadata...\")\nwith open(\"/kaggle/working/latex_tokenizer256.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)\n\nvocab_size = len(tokenizer.word_index) + 1\nwith open(\"/kaggle/working/vocab_size.txt\", \"w\") as f:\n    f.write(str(vocab_size))\n\n# 7. Convert to Tensors and Save (Disk usage check: ~13.5GB total)\nprint(\"Converting to Tensors...\")\n# torch.from_numpy avoids a RAM copy\nimages_tensor = torch.from_numpy(images_array).unsqueeze(1) \ntokens_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n\nprint(\"Saving tensors to disk (this takes a minute)...\")\ntorch.save(images_tensor, \"/kaggle/working/images_train256.pt\")\ntorch.save(tokens_tensor, \"/kaggle/working/tokens_train256.pt\")\n\nprint(\"Done!\")\nprint(f\"Final Vocab Size: {vocab_size}\")\nprint(f\"Image Tensor Shape: {images_tensor.shape}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T22:38:03.850292Z","iopub.execute_input":"2026-02-10T22:38:03.851050Z","iopub.status.idle":"2026-02-10T22:39:40.175053Z","shell.execute_reply.started":"2026-02-10T22:38:03.851019Z","shell.execute_reply":"2026-02-10T22:39:40.173673Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nPre-allocating memory for 40000 images...\nProcessing images and LaTeX strings...\nProgress: 5000/40000\nProgress: 10000/40000\nProgress: 15000/40000\nProgress: 20000/40000\nProgress: 25000/40000\nProgress: 30000/40000\nProgress: 35000/40000\nProgress: 40000/40000\nFitting tokenizer...\nTokenizing and padding sequences...\nSaving metadata...\nConverting to Tensors...\nSaving tensors to disk (this takes a minute)...\nDone!\nFinal Vocab Size: 66\nImage Tensor Shape: torch.Size([40000, 1, 256, 256])\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"ds = load_from_disk(\"/kaggle/input/datasets/dustinp11/mathwriting\")\n\nds_val = ds[\"val\"].select(range(5000))\n\nimages, sequences = [], []\n\ndef preprocess_image(img, target_size=(256, 256)):\n    img = img.convert(\"L\")  # convert to grayscale\n    img = img.resize(target_size)\n    img = np.array(img) / 255.0  # normalize to [0, 1]\n    return img\n\nfor sample in ds_val:\n    img = preprocess_image(sample[\"image\"])\n    images.append(img)\n    sequences.append(sample[\"latex\"])\n\nimages = np.array(images)\nwith open(\"/kaggle/working/latex_tokenizer256.pkl\", \"rb\") as f:\n    tokenizer = pickle.load(f)\n\nSTART_ID = tokenizer.word_index[\"<START>\"]\nEND_ID   = tokenizer.word_index[\"<END>\"]\n\nseqs = tokenizer.texts_to_sequences(sequences)\nseqs = [[START_ID] + s + [END_ID] for s in seqs]\n\npadded_sequences = pad_sequences(seqs, padding=\"post\")\nimages = images[..., np.newaxis]\n\nimages_tensor = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)\ntokens_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n\ntorch.save(images_tensor, \"/kaggle/working/images_val256.pt\")\ntorch.save(tokens_tensor, \"/kaggle/working/tokens_val256.pt\")\nprint(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T22:48:46.467107Z","iopub.execute_input":"2026-02-10T22:48:46.467765Z","iopub.status.idle":"2026-02-10T22:49:02.161635Z","shell.execute_reply.started":"2026-02-10T22:48:46.467734Z","shell.execute_reply":"2026-02-10T22:49:02.160758Z"}},"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 2. Model definition","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, encoder_dim=768):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.enc_to_h = nn.Linear(encoder_dim, hidden_dim)\n\n    def forward(self, x, encoder_features=None, hidden_state=None):\n        x = self.embedding(x)\n        if hidden_state is None:\n            if encoder_features is not None:\n                h0 = torch.tanh(self.enc_to_h(encoder_features)).unsqueeze(0)\n                c0 = torch.zeros_like(h0)\n                output, hidden = self.lstm(x, (h0, c0))\n            else:\n                output, hidden = self.lstm(x)\n        else:\n            output, hidden = self.lstm(x, hidden_state)\n        logits = self.fc(output)\n        return logits, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T22:49:02.163128Z","iopub.execute_input":"2026-02-10T22:49:02.163505Z","iopub.status.idle":"2026-02-10T22:49:02.169886Z","shell.execute_reply.started":"2026-02-10T22:49:02.163476Z","shell.execute_reply":"2026-02-10T22:49:02.168976Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class ViTLatexModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512):\n        super().__init__()\n        from transformers import MobileViTModel\n        self.encoder = MobileViTModel.from_pretrained(\"apple/mobilevit-small\")\n        for param in self.encoder.parameters():\n            param.requires_grad = False\n        encoder_dim = self.encoder.config.neck_hidden_sizes[-1]  # 640\n        self.decoder = Decoder(vocab_size, embed_dim, hidden_dim, encoder_dim)\n    \n    def forward(self, images, targets):\n        encoder_out = self.encoder(images).pooler_output  # Use pooler output instead\n        logits, _ = self.decoder(targets, encoder_features=encoder_out)\n        return logits\n    \n    @torch.no_grad()\n    def generate(self, image, max_len=100, sos_idx=1, eos_idx=2):\n        self.eval()\n        encoder_out = self.encoder(image).pooler_output\n        token = torch.tensor([[sos_idx]], device=image.device)\n        output_tokens = []\n        hidden = None\n        for i in range(max_len):\n            if i == 0:\n                logits, hidden = self.decoder(token, encoder_features=encoder_out)\n            else:\n                logits, hidden = self.decoder(token, hidden_state=hidden)\n            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n            if next_token.item() == eos_idx:\n                break\n            output_tokens.append(next_token.item())\n            token = next_token\n        return output_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T22:49:02.170913Z","iopub.execute_input":"2026-02-10T22:49:02.171358Z","iopub.status.idle":"2026-02-10T22:49:02.186928Z","shell.execute_reply.started":"2026-02-10T22:49:02.171322Z","shell.execute_reply":"2026-02-10T22:49:02.186123Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 3. Training","metadata":{}},{"cell_type":"code","source":"# Load vocab size\nwith open(\"/kaggle/working/vocab_size.txt\") as f:\n    VOCAB_SIZE = int(f.read().strip())\n\n# Hyperparameters\nBATCH_SIZE = 16\nEPOCHS = 50\nLEARNING_RATE = 1e-3\n\n# Load pre-processed tensors (just load, don't convert yet)\nimages_tensor = torch.load(\"/kaggle/working/images_train256.pt\")  # (40000, 1, 256, 256)\ntokens_tensor = torch.load(\"/kaggle/working/tokens_train256.pt\")  # (40000, seq_len)\n\nprint(f\"Images: {images_tensor.shape}, Tokens: {tokens_tensor.shape}, Vocab size: {VOCAB_SIZE}\")\n\n# Create dataset and loader\ndataset = TensorDataset(images_tensor, tokens_tensor)\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Initialize model\nmodel = ViTLatexModel(vocab_size=VOCAB_SIZE).to(DEVICE)\n\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable params: {trainable:,} / {total:,}\")\n\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = torch.optim.Adam(model.decoder.parameters(), lr=LEARNING_RATE)\n\n# Training loop\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    \n    for batch_idx, (imgs, seqs) in enumerate(loader):\n        # Convert grayscale to RGB\n        imgs = imgs.repeat(1, 3, 1, 1)  # (B, 1, 256, 256) -> (B, 3, 256, 256)\n        imgs = imgs.to(DEVICE)\n        seqs = seqs.to(DEVICE)\n        \n        # Teacher forcing\n        input_tokens = seqs[:, :-1]   # (B, seq_len-1)\n        target_tokens = seqs[:, 1:]   # (B, seq_len-1)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        logits = model(imgs, input_tokens)  # (B, seq_len-1, vocab_size)\n        \n        # Compute loss\n        loss = criterion(\n            logits.reshape(-1, VOCAB_SIZE),  # (B * (seq_len-1), vocab_size)\n            target_tokens.reshape(-1)         # (B * (seq_len-1))\n        )\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Print every 100 batches\n        if batch_idx % 100 == 0:\n            print(f\"  Batch {batch_idx}/{len(loader)} | Loss: {loss.item():.4f}\")\n    \n    print(f\"Epoch {epoch + 1}/{EPOCHS} | Avg Loss: {total_loss / len(loader):.4f}\")\n\n# Save model\nSAVE_PATH = \"/kaggle/working/mobilevit_model256.pt\"\ntorch.save({\n    \"model\": model.state_dict()\n}, SAVE_PATH)\n\nprint(f\"Model saved to {SAVE_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T22:49:02.188636Z","iopub.execute_input":"2026-02-10T22:49:02.189023Z","iopub.status.idle":"2026-02-10T23:01:33.570653Z","shell.execute_reply.started":"2026-02-10T22:49:02.188991Z","shell.execute_reply":"2026-02-10T23:01:33.569654Z"}},"outputs":[{"name":"stdout","text":"Images: torch.Size([40000, 1, 256, 256]), Tokens: torch.Size([40000, 159]), Vocab size: 66\nTrainable params: 1,955,906 / 6,893,538\n  Batch 0/2500 | Loss: 4.1920\n  Batch 100/2500 | Loss: 1.4481\n  Batch 200/2500 | Loss: 1.2579\n  Batch 300/2500 | Loss: 1.4113\n  Batch 400/2500 | Loss: 1.2067\n  Batch 500/2500 | Loss: 1.0980\n  Batch 600/2500 | Loss: 1.1920\n  Batch 700/2500 | Loss: 1.3405\n  Batch 800/2500 | Loss: 1.3882\n  Batch 900/2500 | Loss: 1.1913\n  Batch 1000/2500 | Loss: 1.3010\n  Batch 1100/2500 | Loss: 1.0069\n  Batch 1200/2500 | Loss: 1.1402\n  Batch 1300/2500 | Loss: 1.0790\n  Batch 1400/2500 | Loss: 1.1403\n  Batch 1500/2500 | Loss: 1.0927\n  Batch 1600/2500 | Loss: 1.0478\n  Batch 1700/2500 | Loss: 1.2688\n  Batch 1800/2500 | Loss: 1.0919\n  Batch 1900/2500 | Loss: 1.1032\n  Batch 2000/2500 | Loss: 1.0559\n  Batch 2100/2500 | Loss: 0.9818\n  Batch 2200/2500 | Loss: 1.1037\n  Batch 2300/2500 | Loss: 1.0657\n  Batch 2400/2500 | Loss: 1.3239\nEpoch 1/5 | Avg Loss: 1.1597\n  Batch 0/2500 | Loss: 1.2381\n  Batch 100/2500 | Loss: 0.9558\n  Batch 200/2500 | Loss: 0.8411\n  Batch 300/2500 | Loss: 0.9511\n  Batch 400/2500 | Loss: 1.2096\n  Batch 500/2500 | Loss: 0.8805\n  Batch 600/2500 | Loss: 1.4500\n  Batch 700/2500 | Loss: 1.0908\n  Batch 800/2500 | Loss: 1.0941\n  Batch 900/2500 | Loss: 1.0744\n  Batch 1000/2500 | Loss: 0.9274\n  Batch 1100/2500 | Loss: 0.9842\n  Batch 1200/2500 | Loss: 1.0152\n  Batch 1300/2500 | Loss: 1.0085\n  Batch 1400/2500 | Loss: 1.0020\n  Batch 1500/2500 | Loss: 1.5122\n  Batch 1600/2500 | Loss: 1.0901\n  Batch 1700/2500 | Loss: 1.1279\n  Batch 1800/2500 | Loss: 0.8161\n  Batch 1900/2500 | Loss: 0.8254\n  Batch 2000/2500 | Loss: 0.8326\n  Batch 2100/2500 | Loss: 0.8537\n  Batch 2200/2500 | Loss: 0.8631\n  Batch 2300/2500 | Loss: 0.7142\n  Batch 2400/2500 | Loss: 0.8214\nEpoch 2/5 | Avg Loss: 0.9231\n  Batch 0/2500 | Loss: 1.0400\n  Batch 100/2500 | Loss: 0.8370\n  Batch 200/2500 | Loss: 0.8846\n  Batch 300/2500 | Loss: 0.7860\n  Batch 400/2500 | Loss: 0.8636\n  Batch 500/2500 | Loss: 0.9867\n  Batch 600/2500 | Loss: 0.7153\n  Batch 700/2500 | Loss: 0.8079\n  Batch 800/2500 | Loss: 0.7747\n  Batch 900/2500 | Loss: 0.5150\n  Batch 1000/2500 | Loss: 0.9059\n  Batch 1100/2500 | Loss: 0.9258\n  Batch 1200/2500 | Loss: 0.9324\n  Batch 1300/2500 | Loss: 0.6724\n  Batch 1400/2500 | Loss: 0.7918\n  Batch 1500/2500 | Loss: 0.8918\n  Batch 1600/2500 | Loss: 0.7901\n  Batch 1700/2500 | Loss: 0.8588\n  Batch 1800/2500 | Loss: 0.6641\n  Batch 1900/2500 | Loss: 0.7670\n  Batch 2000/2500 | Loss: 0.9955\n  Batch 2100/2500 | Loss: 0.8342\n  Batch 2200/2500 | Loss: 0.9929\n  Batch 2300/2500 | Loss: 0.8595\n  Batch 2400/2500 | Loss: 0.9226\nEpoch 3/5 | Avg Loss: 0.8251\n  Batch 0/2500 | Loss: 0.6671\n  Batch 100/2500 | Loss: 0.5983\n  Batch 200/2500 | Loss: 0.7086\n  Batch 300/2500 | Loss: 0.7615\n  Batch 400/2500 | Loss: 0.7488\n  Batch 500/2500 | Loss: 0.9160\n  Batch 600/2500 | Loss: 0.7190\n  Batch 700/2500 | Loss: 0.8436\n  Batch 800/2500 | Loss: 0.6961\n  Batch 900/2500 | Loss: 0.7175\n  Batch 1000/2500 | Loss: 0.7038\n  Batch 1100/2500 | Loss: 0.7790\n  Batch 1200/2500 | Loss: 0.6356\n  Batch 1300/2500 | Loss: 0.7181\n  Batch 1400/2500 | Loss: 0.8678\n  Batch 1500/2500 | Loss: 0.6965\n  Batch 1600/2500 | Loss: 0.6561\n  Batch 1700/2500 | Loss: 0.8216\n  Batch 1800/2500 | Loss: 0.8126\n  Batch 1900/2500 | Loss: 0.9328\n  Batch 2000/2500 | Loss: 0.7019\n  Batch 2100/2500 | Loss: 0.5252\n  Batch 2200/2500 | Loss: 0.7413\n  Batch 2300/2500 | Loss: 0.7094\n  Batch 2400/2500 | Loss: 0.8676\nEpoch 4/5 | Avg Loss: 0.7518\n  Batch 0/2500 | Loss: 0.6867\n  Batch 100/2500 | Loss: 0.6279\n  Batch 200/2500 | Loss: 0.7414\n  Batch 300/2500 | Loss: 0.7971\n  Batch 400/2500 | Loss: 0.6356\n  Batch 500/2500 | Loss: 0.6865\n  Batch 600/2500 | Loss: 0.6300\n  Batch 700/2500 | Loss: 0.6610\n  Batch 800/2500 | Loss: 0.8071\n  Batch 900/2500 | Loss: 0.6637\n  Batch 1000/2500 | Loss: 0.4698\n  Batch 1100/2500 | Loss: 0.6215\n  Batch 1200/2500 | Loss: 0.8333\n  Batch 1300/2500 | Loss: 0.8005\n  Batch 1400/2500 | Loss: 0.7666\n  Batch 1500/2500 | Loss: 0.7782\n  Batch 1600/2500 | Loss: 0.8837\n  Batch 1700/2500 | Loss: 0.6749\n  Batch 1800/2500 | Loss: 0.6184\n  Batch 1900/2500 | Loss: 0.7402\n  Batch 2000/2500 | Loss: 0.6494\n  Batch 2100/2500 | Loss: 0.6384\n  Batch 2200/2500 | Loss: 0.7247\n  Batch 2300/2500 | Loss: 0.9491\n  Batch 2400/2500 | Loss: 0.6382\nEpoch 5/5 | Avg Loss: 0.6988\nModel saved to /kaggle/working/mobilevit_model256.pt\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 4. Evals","metadata":{}},{"cell_type":"code","source":"def normalized_edit_distance(s1, s2):\n    if len(s1) == 0 and len(s2) == 0:\n        return 0.0\n    if len(s1) == 0 or len(s2) == 0:\n        return 1.0\n    \n    # Levenshtein distance\n    d = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n    for i in range(len(s1) + 1):\n        d[i][0] = i\n    for j in range(len(s2) + 1):\n        d[0][j] = j\n    \n    for i in range(1, len(s1) + 1):\n        for j in range(1, len(s2) + 1):\n            cost = 0 if s1[i-1] == s2[j-1] else 1\n            d[i][j] = min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + cost)\n    \n    return d[len(s1)][len(s2)] / max(len(s1), len(s2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T23:09:56.971588Z","iopub.execute_input":"2026-02-10T23:09:56.971971Z","iopub.status.idle":"2026-02-10T23:09:56.978880Z","shell.execute_reply.started":"2026-02-10T23:09:56.971942Z","shell.execute_reply":"2026-02-10T23:09:56.978032Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom pickle import load\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nwith open(\"/kaggle/working/vocab_size.txt\") as f:\n    VOCAB_SIZE = int(f.read().strip())\n\nSTART_TOKEN = VOCAB_SIZE - 2\nEND_TOKEN = VOCAB_SIZE - 1\nMAX_LEN = 150\n\n# Load model\nmodel = ViTLatexModel(vocab_size=VOCAB_SIZE).to(DEVICE)\ncheckpoint = torch.load(\"/kaggle/working/mobilevit_model256.pt\", map_location=DEVICE)\nmodel.load_state_dict(checkpoint[\"model\"])\nmodel.eval()\n\n# Load validation data\nimages = torch.load(\"/kaggle/working/images_val256.pt\")\ntokens = torch.load(\"/kaggle/working/tokens_val256.pt\")\n\nwith open(\"/kaggle/working/latex_tokenizer256.pkl\", \"rb\") as f:\n    tokenizer = load(f)\n\ninv_vocab = {v: k for k, v in tokenizer.word_index.items()}\n\ndef decode(seq):\n    # Filter out start and end tokens\n    filtered = [t for t in seq if t != START_TOKEN and t != END_TOKEN and t != 0]\n    return \"\".join(inv_vocab.get(t, \"\") for t in filtered)\n\n# Inference\nN = 50\nexact_matches = 0\ntotal_edit_dist = 0.0\n\nprint(f\"Evaluating on {N} test samples...\")\nprint(\"-\" * 60)\n\nfor i in range(N):\n    img = images[i:i+1]  # (1, 1, 256, 256)\n    img = img.repeat(1, 3, 1, 1).to(DEVICE)  # (1, 3, 256, 256)\n    gt_tokens = tokens[i]\n    \n    pred_tokens = model.generate(img, max_len=MAX_LEN, sos_idx=START_TOKEN, eos_idx=END_TOKEN)\n    \n    ground_truth = decode(gt_tokens.tolist())\n    prediction = decode(pred_tokens)\n    \n    is_exact = prediction == ground_truth\n    edit_dist = normalized_edit_distance(prediction, ground_truth)\n    \n    if is_exact:\n        exact_matches += 1\n    total_edit_dist += edit_dist\n    \n    status = \"EXACT\" if is_exact else f\"edit_dist={edit_dist:.4f}\"\n    print(f\"  [{i+1}/{N}] {status}\")\n    print(f\"    GT:   {ground_truth[:80]}\")\n    print(f\"    PRED: {prediction[:80]}\")\n    print(\"-\"*40)\n\naccuracy = exact_matches / N\navg_edit_dist = total_edit_dist / N\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SUMMARY\")\nprint(\"=\" * 60)\nprint(f\"Model:                    MobileViT + LSTM\")\nprint(f\"Samples:                  {N}\")\nprint(f\"Exact match accuracy:     {accuracy:.2%} ({exact_matches}/{N})\")\nprint(f\"Avg normalized edit dist: {avg_edit_dist:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-10T23:10:04.281158Z","iopub.execute_input":"2026-02-10T23:10:04.282136Z","iopub.status.idle":"2026-02-10T23:10:07.032056Z","shell.execute_reply.started":"2026-02-10T23:10:04.282084Z","shell.execute_reply":"2026-02-10T23:10:07.031174Z"}},"outputs":[{"name":"stdout","text":"Evaluating on 50 test samples...\n------------------------------------------------------------\n  [1/50] edit_dist=0.2703\n    GT:   \\frac{\\partial\\psi}{\\partial t}=p\\psi\n    PRED: \\frac{\\partial f}{\\partial z}=0\n----------------------------------------\n  [2/50] edit_dist=0.7586\n    GT:   c_{0}=d/v_{blood}\n    PRED: \\frac{d}{dt}n_{2}=-c_{1}n_{1}\n----------------------------------------\n  [3/50] edit_dist=0.9600\n    GT:   e^{2}/\\hbar c\\approx1/137\n    PRED: \\{\\sqrt{\\pi},2\\pi\\sigma\\}\n----------------------------------------\n  [4/50] edit_dist=0.8214\n    GT:   nassoc(s,\\overline{s})\n    PRED: \\tilde{\\phi}(x)=\\tilde{x}(t)\n----------------------------------------\n  [5/50] edit_dist=0.8065\n    GT:   eq.1\\frac{dv}{dx}=w\n    PRED: \\frac{\\partial f}{\\partial z}=0\n----------------------------------------\n  [6/50] edit_dist=0.7692\n    GT:   \\underline{r}\n    PRED: \\mathbb{n}\n----------------------------------------\n  [7/50] edit_dist=0.8000\n    GT:   \\hat{k}_{g}\n    PRED: c_{\\tilde{\\nu}}\n----------------------------------------\n  [8/50] edit_dist=0.7391\n    GT:   \\int_{0}^{1}f(r)g(r)rdr\n    PRED: \\frac{d}{dx}(x,y)=0\n----------------------------------------\n  [9/50] edit_dist=0.8611\n    GT:   \\sigma_{b}=p\\cdot\\hat{n}_{out}\n    PRED: \\frac{\\partial f}{\\partial x}(x,y)=0\n----------------------------------------\n  [10/50] edit_dist=0.5526\n    GT:   w_{i}^{*}=\\frac{rgq_{bi}}{f_{i}u*^{3}}\n    PRED: m_{n}=\\frac{n_{1}+n_{2}}{n_{2}-n_{2}}\n----------------------------------------\n  [11/50] edit_dist=0.8065\n    GT:   z\\rightarrow-z^{5}+z_{0}\n    PRED: \\tilde{t}_{i},\\tilde{e}_{i}^{a}\n----------------------------------------\n  [12/50] edit_dist=0.7500\n    GT:   1/(d_{i}d_{j})^{1/2}\n    PRED: \\int_{0}^{1}f(x)dx\n----------------------------------------\n  [13/50] edit_dist=0.8095\n    GT:   t_{r}^{\\prime}=t-\\frac{1}{c}|r-r^{\\prime}|\n    PRED: \\frac{d}{dt}n_{2}=-c_{1}n_{1}\n----------------------------------------\n  [14/50] edit_dist=0.2432\n    GT:   \\frac{\\partial h}{\\partial z_{j}^{-}}\n    PRED: \\frac{\\partial f}{\\partial z}\n----------------------------------------\n  [15/50] edit_dist=0.7647\n    GT:   {j^{\\lambda}}^{y}\n    PRED: \\frac{d}{dx}\n----------------------------------------\n  [16/50] edit_dist=0.9000\n    GT:   r_{i}\\rightarrow r=\\prod r_{i}\n    PRED: m=\\int_{0}^{h}e^{x}dx\n----------------------------------------\n  [17/50] edit_dist=0.4800\n    GT:   \\int_{0}^{1}\\phi(p)dp=1\n    PRED: \\int_{0}^{1}f(x)dx=\\infty\n----------------------------------------\n  [18/50] edit_dist=0.8571\n    GT:   h(x)=1_{(0,\\infty)}(x)\n    PRED: \\forall x\\in s,s\\in s,s\\in s\n----------------------------------------\n  [19/50] edit_dist=0.1538\n    GT:   \\tilde{x}_{0}\n    PRED: \\tilde{c}_{n}\n----------------------------------------\n  [20/50] edit_dist=0.3478\n    GT:   \\sqrt{n}/ln(\\sqrt{n})\n    PRED: \\sqrt{4}/455^{\\sqrt{5}}\n----------------------------------------\n  [21/50] edit_dist=0.8333\n    GT:   a^{\\prime}=\\lambda_{1}a+\\lambda_{2}b\n    PRED: a_{1}=\\int pdv\n----------------------------------------\n  [22/50] edit_dist=0.7660\n    GT:   \\lambda=\\mu_{1}^{2}+\\cdot\\cdot\\cdot+\\mu_{k}^{2}\n    PRED: \\prod_{i=1}^{m}q_{i}^{n_{i}}\n----------------------------------------\n  [23/50] edit_dist=0.4815\n    GT:   \\int_{0}^{t}z_{s}ds\n    PRED: \\int_{0}^{1}f^{\\prime}(x)dx\n----------------------------------------\n  [24/50] edit_dist=0.7500\n    GT:   d_{x}(p+ta)=0\n    PRED: \\prod_{i=1}^{m}q(\\beta_{i}x)\n----------------------------------------\n  [25/50] edit_dist=0.8095\n    GT:   \\delta t=\\dot{q}r\n    PRED: (\\sqrt{2})\\circ c^{2}\n----------------------------------------\n  [26/50] edit_dist=0.9286\n    GT:   h_{1}(g;z)=0\n    PRED: \\psi=(\\begin{matrix}0&1\\\\ 1&0\\end{matrix})\n----------------------------------------\n  [27/50] edit_dist=0.7949\n    GT:   \\prod_{i=0}^{k-1}(x-i)^{n}\n    PRED: (\\frac{\\frac{3}{4}}{2})^{\\frac{348}{2}}\n----------------------------------------\n  [28/50] edit_dist=0.8333\n    GT:   v(r)=\\frac{1}{2}m_{0}\\omega^{2}r^{2}\n    PRED: (107+393)^{\\frac{10}{10}}\n----------------------------------------\n  [29/50] edit_dist=0.8205\n    GT:   z_{l}=r_{l}(\\omega)+jx_{l}(\\omega)\n    PRED: \\psi_{0}(x)=\\overline{\\langle x\\rangle}\n----------------------------------------\n  [30/50] edit_dist=0.5000\n    GT:   \\partial_{t}l=\\frac{1}{2}\\nabla^{2}l\n    PRED: \\frac{1}{2}+\\frac{1}{2}+\\frac{1}{2}\n----------------------------------------\n  [31/50] edit_dist=0.6957\n    GT:   \\hat{r}\\cdot\\hat{e}_{3}\n    PRED: \\hat{f}(3)\n----------------------------------------\n  [32/50] edit_dist=0.9000\n    GT:   c_{1},c_{2},...\\in k\n    PRED: \\int_{0}^{1}f(x)dx\n----------------------------------------\n  [33/50] edit_dist=0.7692\n    GT:   \\overline{op_{1}}^{\\prime}\n    PRED: \\sqrt{\\frac{1}{2}}\\sigma\n----------------------------------------\n  [34/50] edit_dist=0.7949\n    GT:   a=(a_{ij})\n    PRED: a=(\\begin{matrix}0&1\\\\ 1&0\\end{matrix})\n----------------------------------------\n  [35/50] edit_dist=0.7143\n    GT:   \\varphi\\vdash\\neg\\psi\n    PRED: \\phi_{t}=\\omega\n----------------------------------------\n  [36/50] edit_dist=0.8696\n    GT:   \\sqrt{n}/ln(\\sqrt{n})\n    PRED: \\prod b_{i}^{\\alpha}(x)\n----------------------------------------\n  [37/50] edit_dist=0.7414\n    GT:   a=\\{a_{ij}\\}_{i,j=0}^{p-1}\n    PRED: a_{i}=\\sum_{j=1}^{n}\\frac{1}{i_{i}}=\\frac{1}{i}\\frac{1}{i}\n----------------------------------------\n  [38/50] edit_dist=0.8652\n    GT:   c=\\frac{6000}{200-2x}\n    PRED: \\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\p\n----------------------------------------\n  [39/50] edit_dist=0.4237\n    GT:   \\frac{\\partial w}{\\partial t}=f_{e}\\cdot v\n    PRED: \\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial x}\n----------------------------------------\n  [40/50] edit_dist=0.1707\n    GT:   a=(\\begin{matrix}1&-2\\\\ 1&-1\\end{matrix})\n    PRED: a=[\\begin{matrix}0&1\\\\ 1&0\\end{matrix}]\n----------------------------------------\n  [41/50] edit_dist=0.1622\n    GT:   (\\begin{matrix}4\\\\ 2\\end{matrix})\n    PRED: (\\begin{matrix}0&0\\\\ 0&0\\end{matrix})\n----------------------------------------\n  [42/50] edit_dist=0.5000\n    GT:   \\frac{x}{2\\pi}=\\frac{y}{2\\pi r}=\n    PRED: \\frac{1}{\\sqrt{1-\\frac{1}{x^{2}}}}\n----------------------------------------\n  [43/50] edit_dist=0.7778\n    GT:   f(r)=-\\frac{k}{r^{3}}\n    PRED: \\frac{1}{\\sqrt{2}}\\simeq0.0\n----------------------------------------\n  [44/50] edit_dist=0.5455\n    GT:   \\lambda_{2}=3-\\sqrt{5}\n    PRED: a=\\sqrt{2}+\\sqrt{3}\n----------------------------------------\n  [45/50] edit_dist=0.3846\n    GT:   \\tilde{q}\n    PRED: \\tilde{c}_{n}\n----------------------------------------\n  [46/50] edit_dist=0.8621\n    GT:   gh^{\\top}=p-p=0\n    PRED: \\frac{d}{dt}n_{2}=-c_{1}n_{1}\n----------------------------------------\n  [47/50] edit_dist=0.9167\n    GT:   -s\n    PRED: \\tilde{\\psi}\n----------------------------------------\n  [48/50] edit_dist=0.3000\n    GT:   \\sqrt{-7}\n    PRED: \\sqrt{\\pi}\n----------------------------------------\n  [49/50] edit_dist=0.8000\n    GT:   x=y=\\frac{i}{\\sqrt{2}}\n    PRED: \\sum_{n=0}^{\\infty}\\frac{1}{n}\n----------------------------------------\n  [50/50] edit_dist=0.7097\n    GT:   \\frac{\\partial u}{\\partial t}=0\n    PRED: \\frac{d^{2}w}{dx^{2}}\n----------------------------------------\n\n============================================================\nSUMMARY\n============================================================\nModel:                    MobileViT + LSTM\nSamples:                  50\nExact match accuracy:     0.00% (0/50)\nAvg normalized edit dist: 0.6774\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}