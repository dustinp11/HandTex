{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Encoder + LSTM Decoder\n",
    "Frozen `google/vit-base-patch16-224` encoder with a trainable LSTM decoder for handwritten math to LaTeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"deepcopy/MathWriting-human\")\n",
    "print(f\"Train: {len(ds['train'])}  Val: {len(ds['val'])}  Test: {len(ds['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to None to use ALL training data\n",
    "NUM_TRAIN = 2000\n",
    "\n",
    "ds_train = ds[\"train\"] if NUM_TRAIN is None else ds[\"train\"].select(range(NUM_TRAIN))\n",
    "print(f\"Using {len(ds_train)} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "\n",
    "images, latex_strings = [], []\n",
    "for sample in ds_train:\n",
    "    img = sample[\"image\"].convert(\"L\").resize((IMG_SIZE, IMG_SIZE))\n",
    "    images.append(np.array(img) / 255.0)\n",
    "    latex_strings.append(sample[\"latex\"])\n",
    "\n",
    "images = np.array(images)\n",
    "print(f\"Images: {images.shape}\")\n",
    "\n",
    "# char-level tokenizer\n",
    "chars = sorted(set(\"\".join(latex_strings)))\n",
    "word_index = {ch: i + 1 for i, ch in enumerate(chars)}\n",
    "idx_to_char = {v: k for k, v in word_index.items()}\n",
    "\n",
    "sequences = [[word_index[ch] for ch in s] for s in latex_strings]\n",
    "max_len = max(len(s) for s in sequences)\n",
    "padded = np.array([s + [0] * (max_len - len(s)) for s in sequences])\n",
    "\n",
    "VOCAB_SIZE = len(word_index) + 1\n",
    "print(f\"Vocab size: {VOCAB_SIZE}, Max seq len: {max_len}\")\n",
    "\n",
    "images_tensor = torch.tensor(images, dtype=torch.float32).unsqueeze(1)  # (N, 1, H, W)\n",
    "tokens_tensor = torch.tensor(padded, dtype=torch.long)\n",
    "print(f\"Images tensor: {images_tensor.shape}\")\n",
    "print(f\"Tokens tensor: {tokens_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare for ViT (3-channel, 224x224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = transforms.Resize((224, 224))\n",
    "\n",
    "images_rgb = images_tensor.repeat(1, 3, 1, 1)\n",
    "images_resized = torch.stack([resize(img) for img in images_rgb])\n",
    "print(f\"ViT input shape: {images_resized.shape}\")\n",
    "\n",
    "dataset = TensorDataset(images_resized, tokens_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, encoder_dim=768):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.enc_to_h = nn.Linear(encoder_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, encoder_features=None, hidden_state=None):\n",
    "        x = self.embedding(x)\n",
    "        if hidden_state is None:\n",
    "            if encoder_features is not None:\n",
    "                h0 = torch.tanh(self.enc_to_h(encoder_features)).unsqueeze(0)\n",
    "                c0 = torch.zeros_like(h0)\n",
    "                output, hidden = self.lstm(x, (h0, c0))\n",
    "            else:\n",
    "                output, hidden = self.lstm(x)\n",
    "        else:\n",
    "            output, hidden = self.lstm(x, hidden_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden\n",
    "\n",
    "\n",
    "class ViTLatexModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        encoder_dim = self.encoder.config.hidden_size\n",
    "        self.decoder = Decoder(vocab_size, embed_dim, hidden_dim, encoder_dim)\n",
    "\n",
    "    def forward(self, images, targets):\n",
    "        encoder_out = self.encoder(images).last_hidden_state[:, 0, :]\n",
    "        logits, _ = self.decoder(targets, encoder_features=encoder_out)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, image, max_len=100, sos_idx=1, eos_idx=2):\n",
    "        self.eval()\n",
    "        encoder_out = self.encoder(image).last_hidden_state[:, 0, :]\n",
    "        token = torch.tensor([[sos_idx]], device=image.device)\n",
    "        output_tokens = []\n",
    "        hidden = None\n",
    "        for i in range(max_len):\n",
    "            if i == 0:\n",
    "                logits, hidden = self.decoder(token, encoder_features=encoder_out)\n",
    "            else:\n",
    "                logits, hidden = self.decoder(token, hidden_state=hidden)\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            if next_token.item() == eos_idx:\n",
    "                break\n",
    "            output_tokens.append(next_token.item())\n",
    "            token = next_token\n",
    "        return output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = ViTLatexModel(vocab_size=VOCAB_SIZE).to(DEVICE)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable:,} / {total:,}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.decoder.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = Path(\"checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (imgs, seqs) in enumerate(loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        seqs = seqs.to(DEVICE)\n",
    "\n",
    "        input_tokens = seqs[:, :-1]\n",
    "        target_tokens = seqs[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs, input_tokens)\n",
    "        loss = criterion(logits.reshape(-1, VOCAB_SIZE), target_tokens.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": avg_loss,\n",
    "    }, checkpoint_dir / f\"vit_epoch_{epoch+1}.pt\")\n",
    "\n",
    "torch.save(model.state_dict(), checkpoint_dir / \"vit_final.pt\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, word_index):\n",
    "        self.word_index = word_index\n",
    "\n",
    "with open(\"latex_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(CharTokenizer(word_index), f)\n",
    "\n",
    "with open(\"vocab_size.txt\", \"w\") as f:\n",
    "    f.write(str(VOCAB_SIZE))\n",
    "\n",
    "print(f\"Saved tokenizer and vocab_size={VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def normalized_edit_distance(pred, target):\n",
    "    m, n = len(pred), len(target)\n",
    "    if m == 0 and n == 0:\n",
    "        return 0.0\n",
    "    dp = list(range(n + 1))\n",
    "    for i in range(1, m + 1):\n",
    "        prev = dp[0]\n",
    "        dp[0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            temp = dp[j]\n",
    "            if pred[i - 1] == target[j - 1]:\n",
    "                dp[j] = prev\n",
    "            else:\n",
    "                dp[j] = 1 + min(dp[j], dp[j - 1], prev)\n",
    "            prev = temp\n",
    "    return dp[n] / max(m, n)\n",
    "\n",
    "\n",
    "NUM_EVAL = 50\n",
    "SEED = 42\n",
    "\n",
    "test_ds = ds[\"test\"]\n",
    "random.seed(SEED)\n",
    "eval_indices = random.sample(range(len(test_ds)), NUM_EVAL)\n",
    "eval_samples = [test_ds[i] for i in eval_indices]\n",
    "\n",
    "resize_eval = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "model.eval()\n",
    "exact_matches = 0\n",
    "total_edit_dist = 0.0\n",
    "\n",
    "print(f\"Evaluating on {NUM_EVAL} test samples...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, sample in enumerate(eval_samples):\n",
    "    ground_truth = sample[\"latex\"]\n",
    "    img = sample[\"image\"].convert(\"L\")\n",
    "    img_tensor = resize_eval(img).repeat(3, 1, 1).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    pred_tokens = model.generate(img_tensor)\n",
    "    prediction = \"\".join(idx_to_char.get(t, \"?\") for t in pred_tokens)\n",
    "\n",
    "    is_exact = prediction == ground_truth\n",
    "    edit_dist = normalized_edit_distance(prediction, ground_truth)\n",
    "\n",
    "    if is_exact:\n",
    "        exact_matches += 1\n",
    "    total_edit_dist += edit_dist\n",
    "\n",
    "    status = \"EXACT\" if is_exact else f\"edit_dist={edit_dist:.4f}\"\n",
    "    print(f\"  [{i+1}/{NUM_EVAL}] {status}\")\n",
    "    print(f\"    GT:   {ground_truth[:80]}\")\n",
    "    print(f\"    Pred: {prediction[:80]}\")\n",
    "\n",
    "accuracy = exact_matches / NUM_EVAL\n",
    "avg_edit_dist = total_edit_dist / NUM_EVAL\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model:                    ViT + LSTM\")\n",
    "print(f\"Samples:                  {NUM_EVAL}\")\n",
    "print(f\"Exact match accuracy:     {accuracy:.2%} ({exact_matches}/{NUM_EVAL})\")\n",
    "print(f\"Avg normalized edit dist: {avg_edit_dist:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
