{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0911019",
   "metadata": {},
   "source": [
    "# ViT Encoder + LSTM Decoder Training\n",
    "Frozen `facebook/dinov2-base` encoder with a trainable LSTM decoder for handwritten math â†’ LaTeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel\n",
    "from pathlib import Path\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from transformers import AutoModel\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc91a6a",
   "metadata": {},
   "source": [
    "## 1. Load & preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b058cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\"deepcopy/MathWriting-human\")\n",
    "num_samples = 40000\n",
    "ds_train = ds[\"train\"].select(range(num_samples))\n",
    "\n",
    "# 2. Pre-allocate Image Array (Saves RAM by avoiding copies)\n",
    "print(f\"Pre-allocating memory for {num_samples} images...\")\n",
    "images_array = np.zeros((num_samples, 224, 224, 3), dtype=np.uint8) # Uses uint8 instead of float to save RAM\n",
    "latex_strings = []\n",
    "\n",
    "# 3. Process Images & Collect Strings\n",
    "print(\"Processing images and LaTeX strings...\")\n",
    "for i in range(num_samples):\n",
    "    sample = ds_train[i]\n",
    "    # Convert and resize directly into the array\n",
    "    img = sample[\"image\"].convert(\"RGB\").resize((224, 224))\n",
    "    images_array[i] = np.array(img, dtype=np.uint8)\n",
    "    latex_strings.append(sample[\"latex\"])\n",
    "    \n",
    "    if (i + 1) % 5000 == 0:\n",
    "        print(f\"Progress: {i + 1}/{num_samples}\")\n",
    "\n",
    "# 4. Setup Tokenizer\n",
    "print(\"Fitting tokenizer...\")\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(latex_strings)\n",
    "\n",
    "# Add special tokens\n",
    "tokenizer.word_index[\"<START>\"] = len(tokenizer.word_index) + 1\n",
    "tokenizer.word_index[\"<END>\"] = len(tokenizer.word_index) + 1\n",
    "tokenizer.index_word[tokenizer.word_index[\"<START>\"]] = \"<START>\"\n",
    "tokenizer.index_word[tokenizer.word_index[\"<END>\"]] = \"<END>\"\n",
    "\n",
    "START_ID = tokenizer.word_index[\"<START>\"]\n",
    "END_ID   = tokenizer.word_index[\"<END>\"]\n",
    "\n",
    "# 5. Sequence Padding\n",
    "print(\"Tokenizing and padding sequences...\")\n",
    "sequences = tokenizer.texts_to_sequences(latex_strings)\n",
    "sequences = [[START_ID] + seq + [END_ID] for seq in sequences]\n",
    "padded_sequences = pad_sequences(sequences, padding=\"post\")\n",
    "\n",
    "# 6. Save Tokenizer and Vocab Info\n",
    "print(\"Saving metadata...\")\n",
    "with open(\"/kaggle/working/latex_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "with open(\"/kaggle/working/vocab_size.txt\", \"w\") as f:\n",
    "    f.write(str(vocab_size))\n",
    "\n",
    "# 7. Convert to Tensors and Save (Disk usage check: ~13.5GB total)\n",
    "print(\"Converting to Tensors...\")\n",
    "# torch.from_numpy avoids a RAM copy\n",
    "images_tensor = torch.from_numpy(images_array).permute(0, 3, 1, 2) \n",
    "tokens_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "print(\"Saving tensors to disk (this takes a minute)...\")\n",
    "torch.save(images_tensor, \"/kaggle/working/images_train.pt\")\n",
    "torch.save(tokens_tensor, \"/kaggle/working/tokens_train.pt\")\n",
    "\n",
    "print(\"Done!\")\n",
    "print(f\"Final Vocab Size: {vocab_size}\")\n",
    "print(f\"Image Tensor Shape: {images_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb23d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"deepcopy/MathWriting-human\")\n",
    "\n",
    "ds_val = ds[\"val\"].select(range(5000))\n",
    "\n",
    "images, sequences = [], []\n",
    "\n",
    "def preprocess_image(img, target_size=(224, 224)):\n",
    "    img = img.convert(\"RGB\")  # convert to grayscale\n",
    "    img = img.resize(target_size)\n",
    "    img = np.array(img) / 255.0  # normalize to [0, 1]\n",
    "    return img\n",
    "\n",
    "for sample in ds_val:\n",
    "    img = preprocess_image(sample[\"image\"])\n",
    "    images.append(img)\n",
    "    sequences.append(sample[\"latex\"])\n",
    "\n",
    "images = np.array(images)\n",
    "with open(\"/kaggle/working/latex_tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "START_ID = tokenizer.word_index[\"<START>\"]\n",
    "END_ID   = tokenizer.word_index[\"<END>\"]\n",
    "\n",
    "seqs = tokenizer.texts_to_sequences(sequences)\n",
    "seqs = [[START_ID] + s + [END_ID] for s in seqs]\n",
    "\n",
    "padded_sequences = pad_sequences(seqs, padding=\"post\")\n",
    "\n",
    "images_tensor = torch.tensor(images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "tokens_tensor = torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "torch.save(images_tensor, \"/kaggle/working/images_val.pt\")\n",
    "torch.save(tokens_tensor, \"/kaggle/working/tokens_val.pt\")\n",
    "print(\"Images:\", images_tensor.shape)\n",
    "print(\"Tokens:\", tokens_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c594604",
   "metadata": {},
   "source": [
    "## 2. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, encoder_dim=768, nhead=8, num_layers=4, dim_feedforward=512, dropout=0.1, max_len=150):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # makes input (B,T,D)\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        if encoder_dim != embed_dim:\n",
    "            self.enc_proj = nn.Linear(encoder_dim, embed_dim)\n",
    "        else:\n",
    "            self.enc_proj = nn.Identity()\n",
    "        \n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, x, enc_mem, tgt_mask=None, enc_mask=None):\n",
    "        B, T = x.shape\n",
    "        positions = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        \n",
    "        x = self.embed_tokens(x) + self.pos_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        enc_mem = self.enc_proj(enc_mem)\n",
    "        \n",
    "        # tgt_mask for causal attention to prevent a token form seeing future tokens only itself and previous \n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(T).to(x.device)\n",
    "        \n",
    "        logits = self.transformer_decoder(\n",
    "            x, \n",
    "            enc_mem, \n",
    "            tgt_mask=tgt_mask, \n",
    "            memory_key_padding_mask=(enc_mask == 0) if enc_mask is not None else None\n",
    "        )\n",
    "        \n",
    "        logits = self.fc_out(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12493315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTLatexModelLoRA(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, nhead=8, num_layers=6,\n",
    "                 lora_r=16, lora_alpha=32, lora_dropout=0.05, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n",
    "        target_modules = [\"query\", \"key\", \"value\", \"dense\", \"fc1\", \"fc2\"]\n",
    "        \n",
    "        lora_cfg = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            target_modules=target_modules,\n",
    "            task_type=\"FEATURE_EXTRACTION\",  # safe default for encoder-only usage\n",
    "        )\n",
    "        self.encoder = get_peft_model(self.encoder, lora_cfg)\n",
    "        encoder_dim = self.encoder.config.hidden_size  # 768\n",
    "\n",
    "        # Decoder embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, 1000, embed_dim))  # max seq length\n",
    "\n",
    "        # Transformer decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # linear projection to vocab\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        # project encoder dim to decoder embedding\n",
    "        if encoder_dim != embed_dim:\n",
    "            self.enc_proj = nn.Linear(encoder_dim, embed_dim)\n",
    "        else:\n",
    "            self.enc_proj = nn.Identity()\n",
    "\n",
    "    def forward(self, images, input_tokens):\n",
    "        \"\"\"\n",
    "        images: (B, 3, H, W)\n",
    "        input_tokens: (B, T)\n",
    "        \"\"\"\n",
    "        enc = self.encoder(pixel_values=images).last_hidden_state  # (B, N, D)\n",
    "        enc = self.enc_proj(enc)  # (B, N, E)\n",
    "\n",
    "        # Decoder embedding + positions\n",
    "        emb = self.embedding(input_tokens) + self.pos_encoder[:, :input_tokens.size(1), :]  # (B, T, E)\n",
    "\n",
    "        # Causal mask for decoder\n",
    "        T = input_tokens.size(1)\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(T).to(images.device)\n",
    "\n",
    "        # Transformer decoder\n",
    "        dec_out = self.transformer_decoder(\n",
    "            tgt=emb,\n",
    "            memory=enc,\n",
    "            tgt_mask=causal_mask\n",
    "        )  # (B, T, E)\n",
    "\n",
    "        # Project to vocab\n",
    "        logits = self.fc_out(dec_out)  # (B, T, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, image, max_len=150, sos_idx=1, eos_idx=2):\n",
    "        self.eval()\n",
    "        device = image.device\n",
    "    \n",
    "        enc = self.encoder(pixel_values=image).last_hidden_state\n",
    "        enc = self.enc_proj(enc)\n",
    "    \n",
    "        tokens = torch.tensor([[sos_idx]], device=device)\n",
    "        for _ in range(max_len):\n",
    "            emb = self.embedding(tokens) + self.pos_encoder[:, :tokens.size(1), :]\n",
    "            causal_mask = nn.Transformer.generate_square_subsequent_mask(tokens.size(1)).to(device)\n",
    "            out = self.transformer_decoder(tgt=emb, memory=enc, tgt_mask=causal_mask)\n",
    "            next_token = self.fc_out(out[:, -1, :]).argmax(dim=-1, keepdim=True)\n",
    "            if next_token.item() == eos_idx:\n",
    "                break\n",
    "            tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    \n",
    "        return tokens[0, 1:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d9576",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f1ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab size\n",
    "with open(\"/kaggle/working/vocab_size.txt\") as f:\n",
    "    VOCAB_SIZE = int(f.read().strip())\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE_DEC = 1e-3  # decoder\n",
    "LEARNING_RATE_ENC = 1e-4  # LoRA adapters in encoder\n",
    "\n",
    "# Load pre-processed tensors (just load, don't convert yet)\n",
    "images_tensor = torch.load(\"/kaggle/working/images_train.pt\")  # (40000, 3, 224, 224)\n",
    "tokens_tensor = torch.load(\"/kaggle/working/tokens_train.pt\")  # (40000, seq_len)\n",
    "\n",
    "print(f\"Images: {images_tensor.shape}, Tokens: {tokens_tensor.shape}, Vocab size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Create dataset and loader\n",
    "dataset = TensorDataset(images_tensor, tokens_tensor)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "model = ViTLatexModelLoRA(vocab_size=VOCAB_SIZE).to(DEVICE)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable:,} / {total:,}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "#optimizer = torch.optim.Adam(model.decoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": model.transformer_decoder.parameters(), \"lr\": LEARNING_RATE_DEC},\n",
    "        {\"params\": model.encoder.parameters(), \"lr\": LEARNING_RATE_ENC},  # LoRA adapters\n",
    "    ],\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1,3,1,1)\n",
    "std  = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1,3,1,1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (imgs, seqs) in enumerate(loader):\n",
    "        # Convert grayscale to RGB\n",
    "        imgs = imgs.to(DEVICE, dtype=torch.float32) / 255.0\n",
    "        imgs = (imgs - mean) / std\n",
    "        \n",
    "        seqs = seqs.to(DEVICE)\n",
    "        \n",
    "        # Teacher forcing\n",
    "        input_tokens = seqs[:, :-1]   # (B, seq_len-1)\n",
    "        target_tokens = seqs[:, 1:]   # (B, seq_len-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(imgs, input_tokens)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, VOCAB_SIZE),  # (B * (seq_len-1), vocab_size)\n",
    "            target_tokens.reshape(-1)         # (B * (seq_len-1))\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(loader)} | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} | Avg Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "# Save model\n",
    "SAVE_PATH = \"/kaggle/working/dinov2_attn_lora.pt\"\n",
    "torch.save({\n",
    "    \"model\": model.state_dict()\n",
    "}, SAVE_PATH)\n",
    "\n",
    "print(f\"Model saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365796ad",
   "metadata": {},
   "source": [
    "## 4. Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_edit_distance(s1, s2):\n",
    "    if len(s1) == 0 and len(s2) == 0:\n",
    "        return 0.0\n",
    "    if len(s1) == 0 or len(s2) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    # Levenshtein distance\n",
    "    d = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n",
    "    for i in range(len(s1) + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(len(s2) + 1):\n",
    "        d[0][j] = j\n",
    "    \n",
    "    for i in range(1, len(s1) + 1):\n",
    "        for j in range(1, len(s2) + 1):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            d[i][j] = min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + cost)\n",
    "    \n",
    "    return d[len(s1)][len(s2)] / max(len(s1), len(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b2bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pickle import load\n",
    "\n",
    "DATA = \"/kaggle/input/datasets/martinvu7/vit-transformer2\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(f\"/kaggle/working/vocab_size.txt\") as f:\n",
    "    VOCAB_SIZE = int(f.read().strip())\n",
    "\n",
    "START_TOKEN = VOCAB_SIZE - 2\n",
    "END_TOKEN = VOCAB_SIZE - 1\n",
    "MAX_LEN = 150\n",
    "\n",
    "# Load model\n",
    "model = ViTLatexModelLoRA(vocab_size=VOCAB_SIZE).to(DEVICE)\n",
    "checkpoint = torch.load(f\"{DATA}/dinov2_attn_lora.pt\", map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "# Load validation data\n",
    "images = torch.load(f\"/kaggle/working/images_val.pt\")\n",
    "tokens = torch.load(f\"/kaggle/working/tokens_val.pt\")\n",
    "\n",
    "with open(f\"/kaggle/working/latex_tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = load(f)\n",
    "\n",
    "inv_vocab = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "def decode(seq):\n",
    "    # Filter out start and end tokens\n",
    "    filtered = [t for t in seq if t != START_TOKEN and t != END_TOKEN and t != 0]\n",
    "    return \"\".join(inv_vocab.get(t, \"\") for t in filtered)\n",
    "\n",
    "# Inference\n",
    "N = 5000\n",
    "exact_matches = 0\n",
    "total_edit_dist = 0.0\n",
    "print(f\"Evaluating on {N} test samples...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(N):\n",
    "    img = images[i:i+1].to(DEVICE)  # (1, 3, 224, 224)\n",
    "    gt_tokens = tokens[i]\n",
    "    \n",
    "    pred_tokens = model.generate(img, max_len=MAX_LEN, sos_idx=START_TOKEN, eos_idx=END_TOKEN)\n",
    "    \n",
    "    ground_truth = decode(gt_tokens.tolist())\n",
    "    prediction = decode(pred_tokens)\n",
    "    \n",
    "    is_exact = prediction == ground_truth\n",
    "    edit_dist = normalized_edit_distance(prediction, ground_truth)\n",
    "    \n",
    "    if is_exact:\n",
    "        exact_matches += 1\n",
    "    total_edit_dist += edit_dist\n",
    "    \n",
    "    # status = \"EXACT\" if is_exact else f\"edit_dist={edit_dist:.4f}\"\n",
    "    # print(f\"  [{i+1}/{N}] {status}\")\n",
    "    # print(f\"    GT:   {ground_truth[:80]}\")\n",
    "    # print(f\"    PRED: {prediction[:80]}\")\n",
    "    # print(\"-\"*40)\n",
    "\n",
    "accuracy = exact_matches / N\n",
    "avg_edit_dist = total_edit_dist / N\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model:                    DinoV2 + Transformer\")\n",
    "print(f\"Samples:                  {N}\")\n",
    "print(f\"Exact match accuracy:     {accuracy:.2%} ({exact_matches}/{N})\")\n",
    "print(f\"Avg normalized edit dist: {avg_edit_dist:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
