{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport sys\nimport pickle\nfrom pathlib import Path\nfrom datasets import load_dataset\n\nsys.path.insert(0, \"models\")\n\nDEVICE    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nARTIFACTS = Path(\"artifacts\")\nCACHE     = Path(\"../cache\")\nBACKEND   = Path(\"../backend/artifacts\")\n\ndef normalized_edit_distance(s1, s2):\n    if len(s1) == 0 and len(s2) == 0: return 0.0\n    if len(s1) == 0 or  len(s2) == 0: return 1.0\n    d = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n    for i in range(len(s1) + 1): d[i][0] = i\n    for j in range(len(s2) + 1): d[0][j] = j\n    for i in range(1, len(s1) + 1):\n        for j in range(1, len(s2) + 1):\n            cost = 0 if s1[i-1] == s2[j-1] else 1\n            d[i][j] = min(d[i-1][j]+1, d[i][j-1]+1, d[i-1][j-1]+cost)\n    return d[len(s1)][len(s2)] / max(len(s1), len(s2))\n\nprint(f\"Device: {DEVICE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "N = 50\nds = load_dataset(\"deepcopy/MathWriting-human\")\nds_test = ds[\"val\"].select(range(N))\ngt_latex = [s[\"latex\"] for s in ds_test]\nprint(f\"Loaded {N} samples from validation split\")\nprint(f\"Available splits: {list(ds.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Eval 1 â€” DINOv2 + LSTM (Bahdanau Attention)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from vit_lora_lstm_attn import ViTLatexModelLoRA as LSTMModel\n\n# Tokenizer + special token indices (matches backend/app.py exactly)\nwith open(BACKEND / \"latex_tokenizer256.pkl\", \"rb\") as f:\n    lstm_tok = pickle.load(f)\nwith open(BACKEND / \"vocab_size.txt\") as f:\n    VOCAB_SIZE = int(f.read().strip())\nLSTM_START = VOCAB_SIZE - 2  # 64\nLSTM_END   = VOCAB_SIZE - 1  # 65\nlstm_inv_vocab = {v: k for k, v in lstm_tok.word_index.items()}\nprint(f\"vocab_size={VOCAB_SIZE}, START={LSTM_START}, END={LSTM_END}\")\n\ndef lstm_decode(seq):\n    return \"\".join(lstm_inv_vocab.get(t, \"\") for t in seq\n                   if t not in (0, LSTM_START, LSTM_END))\n\n# Load model\nlstm_model = LSTMModel(vocab_size=VOCAB_SIZE, lora_r=16).to(DEVICE)\nckpt = torch.load(ARTIFACTS / \"dinov2_attn_lora256.pt\", map_location=DEVICE, weights_only=False)\nlstm_model.load_state_dict(ckpt[\"model\"])\nlstm_model.eval()\nprint(\"LSTM model loaded.\")\n\n# Preprocess: RGB 224x224, /255, ImageNet norm (matches training)\nMEAN = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1, 3, 1, 1)\nSTD  = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1, 3, 1, 1)\n\nlstm_images = []\nfor s in ds_test:\n    img = np.array(s[\"image\"].convert(\"RGB\").resize((224, 224)), dtype=np.uint8)\n    lstm_images.append(img)\nlstm_images = torch.from_numpy(np.array(lstm_images)).permute(0, 3, 1, 2)  # uint8 (N,3,224,224)\nprint(f\"Preprocessed {N} images.\")"
  },
  {
   "cell_type": "code",
   "source": "exact = 0\ntotal_ed = 0.0\nprint(f\"Evaluating LSTM on {N} test samples...\")\nprint(\"-\" * 60)\n\nfor i in range(N):\n    img = lstm_images[i:i+1].to(DEVICE, dtype=torch.float32) / 255.0\n    img = (img - MEAN) / STD\n    pred_tokens  = lstm_model.generate(img, max_len=150, sos_idx=LSTM_START, eos_idx=LSTM_END)\n    prediction   = lstm_decode(pred_tokens)\n    ground_truth = gt_latex[i]\n\n    is_exact  = prediction == ground_truth\n    edit_dist = normalized_edit_distance(prediction, ground_truth)\n    if is_exact: exact += 1\n    total_ed += edit_dist\n\n    status = \"EXACT\" if is_exact else f\"edit_dist={edit_dist:.4f}\"\n    print(f\"  [{i+1:2d}/{N}] {status}\")\n    print(f\"    GT:   {ground_truth[:80]}\")\n    print(f\"    PRED: {prediction[:80]}\")\n    print(\"-\" * 40)\n\nprint(\"=\" * 60)\nprint(f\"[LSTM] Exact match:         {exact/N:.2%} ({exact}/{N})\")\nprint(f\"[LSTM] Avg normalized edit: {total_ed/N:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import importlib\nif \"vit_transformer\" in sys.modules:\n    importlib.reload(sys.modules[\"vit_transformer\"])\nimport vit_transformer as vt_mod\n\n# Load vocab\nwith open(CACHE / \"latex_vocab.pkl\", \"rb\") as f:\n    tr_vocab = pickle.load(f)\ntr_inv_vocab = {v: k for k, v in tr_vocab.items()}\nvt_mod.inv_vocab = tr_inv_vocab  # inject module global for brace depth tracking\nTR_VOCAB_SIZE = len(tr_vocab)\nTR_START, TR_END = 1, 2\nprint(f\"Transformer vocab_size={TR_VOCAB_SIZE}\")\n\ndef tr_decode(seq):\n    return \"\".join(tr_inv_vocab.get(t, \"\") for t in seq if t not in (0, TR_START, TR_END))\n\n# Load model\ntr_model = vt_mod.ViTLatexModelLoRA(vocab_size=TR_VOCAB_SIZE, decoder_type=\"latex_transformer\").to(DEVICE)\ntr_ckpt = torch.load(ARTIFACTS / \"dinov2_latex_transformer.pt\", map_location=DEVICE, weights_only=False)\ntr_state = tr_ckpt[\"model\"] if isinstance(tr_ckpt, dict) and \"model\" in tr_ckpt else tr_ckpt\ntr_model.load_state_dict(tr_state)\ntr_model.eval()\nprint(\"Transformer model loaded.\")\n\n# Preprocess: RGB 224x224, stored as uint8 (norm applied per-image in eval loop)\nTR_MEAN = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1, 3, 1, 1)\nTR_STD  = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1, 3, 1, 1)\n\ntr_images = []\nfor s in ds_test:\n    img = np.array(s[\"image\"].convert(\"RGB\").resize((224, 224)), dtype=np.uint8)\n    tr_images.append(img)\ntr_images = torch.from_numpy(np.array(tr_images)).permute(0, 3, 1, 2)  # uint8 (N,3,224,224)\nprint(f\"Preprocessed {N} images.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "tr_exact = 0\ntr_total_ed = 0.0\nprint(f\"Evaluating Transformer on {N} test samples...\")\nprint(\"-\" * 60)\n\nfor i in range(N):\n    img = tr_images[i:i+1].to(DEVICE, dtype=torch.float32) / 255.0\n    img = (img - TR_MEAN) / TR_STD\n    pred_tokens  = tr_model.generate(img, max_len=128, sos_idx=TR_START, eos_idx=TR_END)\n    prediction   = tr_decode(pred_tokens)\n    ground_truth = gt_latex[i]\n\n    is_exact  = prediction == ground_truth\n    edit_dist = normalized_edit_distance(prediction, ground_truth)\n    if is_exact: tr_exact += 1\n    tr_total_ed += edit_dist\n\n    status = \"EXACT\" if is_exact else f\"edit_dist={edit_dist:.4f}\"\n    print(f\"  [{i+1:2d}/{N}] {status}\")\n    print(f\"    GT:   {ground_truth[:80]}\")\n    print(f\"    PRED: {prediction[:80]}\")\n    print(\"-\" * 40)\n\nprint(\"=\" * 60)\nprint(f\"[Transformer] Exact match:         {tr_exact/N:.2%} ({tr_exact}/{N})\")\nprint(f\"[Transformer] Avg normalized edit: {tr_total_ed/N:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs175_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}