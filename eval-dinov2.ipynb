{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom transformers import AutoModel\nfrom peft import LoraConfig, get_peft_model\nfrom tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\nfrom datasets import load_dataset\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef normalized_edit_distance(s1, s2):\n    if len(s1) == 0 and len(s2) == 0: return 0.0\n    if len(s1) == 0 or  len(s2) == 0: return 1.0\n    d = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]\n    for i in range(len(s1) + 1): d[i][0] = i\n    for j in range(len(s2) + 1): d[0][j] = j\n    for i in range(1, len(s1) + 1):\n        for j in range(1, len(s2) + 1):\n            cost = 0 if s1[i-1] == s2[j-1] else 1\n            d[i][j] = min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + cost)\n    return d[len(s1)][len(s2)] / max(len(s1), len(s2))\n\nclass BahdanauAttention(nn.Module):\n    def __init__(self, hidden_dim, encoder_dim):\n        super().__init__()\n        self.W_h = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.W_e = nn.Linear(encoder_dim, hidden_dim, bias=False)\n        self.v   = nn.Linear(hidden_dim, 1, bias=False)\n\n    def forward(self, h, enc_mem):\n        energy = torch.tanh(self.W_h(h).unsqueeze(1) + self.W_e(enc_mem))\n        alpha = F.softmax(self.v(energy).squeeze(-1), dim=-1)\n        context = torch.bmm(alpha.unsqueeze(1), enc_mem).squeeze(1)\n        return context, alpha\n\nclass AttnDecoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, encoder_dim=768, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.attn = BahdanauAttention(hidden_dim, encoder_dim)\n        self.lstm_cell = nn.LSTMCell(embed_dim + encoder_dim, hidden_dim)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        self.enc_to_h = nn.Linear(encoder_dim, hidden_dim)\n        self.enc_to_c = nn.Linear(encoder_dim, hidden_dim)\n\n    def forward(self, input_ids, enc_mem, hidden_state=None):\n        emb = self.dropout(self.embedding(input_ids))\n        if hidden_state is None:\n            pooled = enc_mem.mean(dim=1)\n            h = torch.tanh(self.enc_to_h(pooled))\n            c = torch.tanh(self.enc_to_c(pooled))\n        else:\n            h, c = hidden_state\n        logits = []\n        for t in range(input_ids.shape[1]):\n            context, _ = self.attn(h, enc_mem)\n            h, c = self.lstm_cell(torch.cat([emb[:, t, :], context], dim=1), (h, c))\n            logits.append(self.fc(h).unsqueeze(1))\n        return torch.cat(logits, dim=1), (h, c)\n\nclass ViTLatexModelLoRA(nn.Module):\n    def __init__(self, vocab_size, lora_r=16, lora_alpha=32, lora_dropout=0.05):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(\"facebook/dinov2-base\")\n        lora_cfg = LoraConfig(\n            r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, bias=\"none\",\n            target_modules=[\"query\", \"key\", \"value\", \"dense\", \"fc1\", \"fc2\"],\n            task_type=\"FEATURE_EXTRACTION\",\n        )\n        self.encoder = get_peft_model(self.encoder, lora_cfg)\n        self.decoder = AttnDecoder(vocab_size, encoder_dim=self.encoder.config.hidden_size)\n\n    def forward(self, images, input_tokens):\n        enc_mem = self.encoder(pixel_values=images).last_hidden_state[:, 1:, :]\n        logits, _ = self.decoder(input_tokens, enc_mem)\n        return logits\n\n    @torch.no_grad()\n    def generate(self, image, max_len=150, sos_idx=1, eos_idx=2):\n        self.eval()\n        enc_mem = self.encoder(pixel_values=image).last_hidden_state[:, 1:, :]\n        token = torch.tensor([[sos_idx]], device=image.device)\n        output_tokens, hidden = [], None\n        for _ in range(max_len):\n            logits, hidden = self.decoder(token, enc_mem, hidden_state=hidden)\n            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n            if next_token.item() == eos_idx: break\n            output_tokens.append(next_token.item())\n            token = next_token\n        return output_tokens"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "N = 50\nds = load_dataset(\"deepcopy/MathWriting-human\")\nds_train = ds[\"train\"].select(range(40000))\nds_test  = ds[\"test\"].select(range(N))\n\n# Rebuild the same Keras char-level tokenizer used during training\ntokenizer = KerasTokenizer(char_level=True)\ntokenizer.fit_on_texts([s[\"latex\"] for s in ds_train])\ntokenizer.word_index[\"<START>\"] = len(tokenizer.word_index) + 1\ntokenizer.word_index[\"<END>\"]   = len(tokenizer.word_index) + 1\ntokenizer.index_word = {v: k for k, v in tokenizer.word_index.items()}\n\nVOCAB_SIZE   = len(tokenizer.word_index) + 1  # 66\nSTART_TOKEN  = tokenizer.word_index[\"<START>\"]  # 64\nEND_TOKEN    = tokenizer.word_index[\"<END>\"]    # 65\nprint(f\"vocab_size={VOCAB_SIZE}, START={START_TOKEN}, END={END_TOKEN}\")\n\n# Preprocess test images: grayscale 256x256, [0,1] float\nimages, gt_latex = [], []\nfor sample in ds_test:\n    img = np.array(sample[\"image\"].convert(\"L\").resize((256, 256)), dtype=np.float32) / 255.0\n    images.append(img)\n    gt_latex.append(sample[\"latex\"])\nimages = torch.tensor(np.array(images)).unsqueeze(1)  # (N, 1, 256, 256)\n\ndef decode(seq):\n    return \"\".join(tokenizer.index_word.get(t, \"\") for t in seq\n                   if t not in (0, START_TOKEN, END_TOKEN))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "checkpoint = torch.load(\"dinov2_attn_lora256.pt\", map_location=DEVICE)\nmodel = ViTLatexModelLoRA(vocab_size=VOCAB_SIZE, lora_r=16).to(DEVICE)\nmodel.load_state_dict(checkpoint[\"model\"])\nmodel.eval()\nprint(\"Model loaded.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "exact_matches = 0\ntotal_edit_dist = 0.0\n\nprint(f\"Evaluating on {N} test samples...\")\nprint(\"-\" * 60)\n\nfor i in range(N):\n    # grayscale [0,1] -> repeat to RGB (no ImageNet norm, matches original eval)\n    img = images[i:i+1].repeat(1, 3, 1, 1).to(DEVICE)\n\n    pred_tokens  = model.generate(img, max_len=150, sos_idx=START_TOKEN, eos_idx=END_TOKEN)\n    ground_truth = gt_latex[i]\n    prediction   = decode(pred_tokens)\n\n    is_exact  = prediction == ground_truth\n    edit_dist = normalized_edit_distance(prediction, ground_truth)\n    if is_exact: exact_matches += 1\n    total_edit_dist += edit_dist\n\n    status = \"EXACT\" if is_exact else f\"edit_dist={edit_dist:.4f}\"\n    print(f\"  [{i+1}/{N}] {status}\")\n    print(f\"    GT:   {ground_truth[:80]}\")\n    print(f\"    PRED: {prediction[:80]}\")\n    print(\"-\" * 40)\n\nprint(\"=\" * 60)\nprint(f\"Exact match accuracy:     {exact_matches/N:.2%} ({exact_matches}/{N})\")\nprint(f\"Avg normalized edit dist: {total_edit_dist/N:.4f}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs175_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}